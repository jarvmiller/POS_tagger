{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags = [u'CC', u'CD', u'DT', u'EX', u'FW', u'IN', u'JJ', u'JJR', u'JJS', u'LS', u'MD',\n",
    "#             u'NN', u'NP', u'NPS', u'NNS', u'PDT', u'POS', u'PP', u'PP$', u'RB', u'RBR',\n",
    "#             u'RBS', u'RP', u'SYM', u'TO', u'UH', u'VB', u'VBD', u'VBG', u'VBN', u'VBP', u'VBZ',\n",
    "#             u'WDT', u'WP', u'WP$', u'WRB']\n",
    "# with open(\"POS.train\", 'r') as train: \n",
    "#     text = train.read()\n",
    "#     pos_tags_single = re.findall('(/[A-Z]{2,3}\\$*)', text)\n",
    "# #     pos_tags = re.findall('(/[A-Z]{2,3}\\$*\\|*[A-Z]{2,3}\\$*)', text)\n",
    "# # pos_tags = set(pos_tags).union(set(pos_tags_single))\n",
    "\n",
    "# # pos_tags = [x[1:] for x in pos_tags]\n",
    "# print(pos_tags,len(pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"POS.train\"\n",
    "\n",
    "unigram = []\n",
    "bigram = []\n",
    "bigram_count= {}\n",
    "i = 0\n",
    "\n",
    "with open (filename,'r') as training:\n",
    "    lines = training.readlines()\n",
    "    start = len(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        line.append('start')\n",
    "        for item in line:\n",
    "            tag = item.split('/')[-1]\n",
    "            # if it's an ambiguous tag we get the second one\n",
    "            if \"|\" in tag:\n",
    "                tag = tag.split(\"|\")[-1]\n",
    "\n",
    "            unigram.append(tag)\n",
    "\n",
    "    for word in unigram:\n",
    "\n",
    "        if i < len(unigram)-1:\n",
    "            bigram.append(unigram[i] + ' '+unigram[i+1])\n",
    "            i+=1\n",
    "\n",
    "    new_bigram = [x for x in bigram if not x.endswith(\"start\")]\n",
    "\n",
    "    for item in new_bigram:       \n",
    "        if item not in bigram_count:\n",
    "            bigram_count[item]=1\n",
    "        else:\n",
    "            bigram_count[item]+=1\n",
    "\n",
    "# tag_frequency, pos_word_prob \n",
    "pos = []\n",
    "with open (filename,'r') as training:\n",
    "    string_r = training.read()\n",
    "\n",
    "    splited_string = string_r.split()\n",
    "\n",
    "    for item in splited_string:\n",
    "        pos.append(item.split('/')[-1]) #Choose the last one when there are multiple slashes.\n",
    "\n",
    "    tag_frequency = Counter(pos)\n",
    "    tag_frequency.update({'start':start})\n",
    "\n",
    "    pos_word_count = Counter(splited_string)\n",
    "    pos_word_prob = pos_word_count.copy()\n",
    "\n",
    "    for item in pos_word_count:\n",
    "        pos_word_prob[item] = pos_word_count[item]/float(tag_frequency[item.split('/')[-1]])\n",
    "\n",
    "    # bigram_prob\n",
    "bigram_prob = {}\n",
    "for key,value in bigram_count.items():\n",
    "    bigram_prob[key]=value/float(tag_frequency[key.split()[0]])\n",
    "\n",
    "#     return unigram, bigram_prob, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 0\n",
    "for key, val in pos_word_count.items():\n",
    "    if val == 1:\n",
    "        N1 += 1\n",
    "turing = N1 / sum(pos_word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, bigram_prob, lines = parse_train()\n",
    "\n",
    "all_tags = []\n",
    "for key in bigram_prob.keys(): # in order i-1 i\n",
    "    all_tags.extend(key.split())\n",
    "\n",
    "# store unique tags\n",
    "tag_list = list(set(all_tags))\n",
    "# bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the testing file\n",
    "test_unigram = []\n",
    "test_path = 'POS.test'\n",
    "with open (test_path,'r') as testing:\n",
    "    test_lines = testing.readlines()\n",
    "    start = len(test_lines)\n",
    "\n",
    "    for line in test_lines:\n",
    "        line = line.split()\n",
    "        line.append('start')\n",
    "        for item in line:\n",
    "            tag = item.split('/')[-1]\n",
    "            # if it's an ambiguous tag we get the second one\n",
    "            if \"|\" in tag:\n",
    "                tag = tag.split(\"|\")[-1]\n",
    "\n",
    "            test_unigram.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init\n",
    "# for word in text.split():\n",
    "#     val = word.split('/')[0]\n",
    "# #     print(word, val)\n",
    "#     init_word_vec = []\n",
    "#     for tag in pos_tags:\n",
    "#         wi_ti = val + '/' + tag\n",
    "#         print(wi_ti)\n",
    "#         ti_tim1 = tag + '/<s>'\n",
    "#         try:\n",
    "# #             print(p_wi_ti[wi_ti])\n",
    "#             init_word_vec.append(p_wi_ti[wi_ti]*p_ti_tim1[ti_tim1])\n",
    "#             # if wi_ti\n",
    "#         except:\n",
    "#             init_word_vec.append(0)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_vec = [init_word_vec]\n",
    "# back_vec=[0]\n",
    "# print(total_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# for word_ind, word in enumerate(text.split()[1:100]): # for each word\n",
    "#     val = word.split('/')[0] \n",
    "#     word_vec = []\n",
    "#     for pos_ind, tag in enumerate(pos_tags): # for each tag\n",
    "#         # get key for p(wi|ti)\n",
    "#         wi_ti = val + '/' + tag\n",
    "#         tag_probs = []\n",
    "#         for cond_tag in pos_tags: \n",
    "#             # get key for p(ti|t_j) where j is over all tags\n",
    "#             ti_tim1 = tag + '/' + cond_tag\n",
    "#             try:\n",
    "#                 tag_probs.append(p_ti_tim1[ti_tim1])\n",
    "#             except:\n",
    "#                 tag_probs.append(0)\n",
    "        \n",
    "#         # previous score vector * vector of conditional tag probs\n",
    "#         vec_to_max = np.array(total_vec[word_ind]) * np.array(tag_probs)\n",
    "#         try:\n",
    "#             word_vec.append(p_wi_ti[wi_ti] * max(vec_to_max))\n",
    "#         except:\n",
    "#             word_vec.append(0)\n",
    "#     back_vec.append(np.argmax(vec_to_max))\n",
    "#     total_vec.append(word_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(total_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# ind = 0\n",
    "# word_map = OrderedDict()\n",
    "# for line in lines:\n",
    "#     for pair in line.split():\n",
    "#         word = pair.split('/')[0]\n",
    "#         words.append(word)\n",
    "# for word in words:\n",
    "#     if word not in word_map:\n",
    "#         word_map[word] = ind\n",
    "#         ind += 1\n",
    "# # word_map = create_word_mapping(lines)\n",
    "# # word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((46, 46), 46)\n"
     ]
    }
   ],
   "source": [
    "trans_p = np.zeros(shape=(len(tag_list), len(tag_list)))\n",
    "print(trans_p.shape, len(trans_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_to_share = 0\n",
    "# for tag in enumerate(tag_list):\n",
    "#     for cond_tag in enumerate(tag_list):\n",
    "#         ti_tim1 = cond_tag + ' ' + tag\n",
    "#         try:\n",
    "            \n",
    "\n",
    "for col_ind, tag in enumerate(tag_list):\n",
    "    for row_ind, cond_tag in enumerate(tag_list):\n",
    "\n",
    "        ti_tim1 = cond_tag + ' ' + tag\n",
    "        try:\n",
    "#             print('trans_p at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             ti_tim1, bigram_prob[ti_tim1]))\n",
    "            trans_p[row_ind, col_ind] = bigram_prob[ti_tim1]\n",
    "        except:\n",
    "#             num_to_share += 1\n",
    "                pass\n",
    "#             print('trans_p at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             ti_tim1, 0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_matrix = np.zeros(shape=(len(pos_tags), len(text.split())))\n",
    "# print(obs_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe do this for each sentence?? Also need to update p_wi_ti and p_ti|tim1 for punctuation\n",
    "# for line in lines[0:1]:\n",
    "# #     print(line)\n",
    "#     sentence = []\n",
    "#     # need to do this \n",
    "#     for col_ind, txt in enumerate(line.split()):\n",
    "#         word = txt.split('/')[0]\n",
    "\n",
    "\n",
    "#         sentence.append(word)\n",
    "#         for row_ind, pos in enumerate(pos_tags):\n",
    "#             wi_ti = word + '/' + pos\n",
    "#             try:\n",
    "#                 print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                                 wi_ti, p_wi_ti[wi_ti]))\n",
    "#                 obs_matrix[row_ind, col_ind] = p_wi_ti[wi_ti]\n",
    "#             except:\n",
    "#                 print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                                 wi_ti, 0))       \n",
    "#                 pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = []\n",
    "# for col_ind, txt in enumerate(text.split()[0:17]):\n",
    "#     word = txt.split('/')[0]\n",
    "# #     if word.strip(string.punctuation) == '':\n",
    "# #         continue\n",
    "\n",
    "#     sentence.append(word)\n",
    "#     for row_ind, pos in enumerate(pos_tags):\n",
    "#         wi_ti = word + '/' + pos\n",
    "#         try:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             wi_ti, p_wi_ti[wi_ti]))\n",
    "#             obs_matrix[row_ind, col_ind] = p_wi_ti[wi_ti]\n",
    "#         except:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             wi_ti, 0))       \n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = np.zeros(shape=(len(tag_list),1))\n",
    "# print(init_state.shape, sentence)\n",
    "# obs_matrix[:,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(obs_matrix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, tag in enumerate(tag_list):\n",
    "    try:\n",
    "#         print('init_state at {}, is {} w/ val {}'.format(ind, 'start' + \" \"+ tag,\n",
    "#                                                          bigram_prob['start' + ' ' + tag]))       \n",
    "        init_state[ind] = bigram_prob['start' + ' ' + tag]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, initialProb, transProb, obsProb):\n",
    "        self.N = initialProb.shape[0]\n",
    "        self.initialProb = initialProb\n",
    "        self.transProb = transProb\n",
    "        self.obsProb = obsProb\n",
    "        assert self.initialProb.shape == (self.N, 1)\n",
    "        assert self.transProb.shape == (self.N, self.N)\n",
    "        assert self.obsProb.shape[0] == self.N\n",
    "\n",
    "    def Obs(self, obs):\n",
    "        return self.obsProb[:, obs, None]\n",
    "\n",
    "    def Decode(self, obs):\n",
    "        trellis = np.zeros((self.N, len(obs)))\n",
    "        backpt = np.ones((self.N, len(obs)), 'int32') * -1\n",
    "\n",
    "        # initialization\n",
    "        # first row of trans matrix time first column of \n",
    "        # emission matrix\n",
    "        trellis[:, 0] = np.squeeze(self.initialProb * self.Obs(obs[0]))\n",
    "\n",
    "        for t in xrange(1, len(obs)):\n",
    "            # trellis[:, t-1, None] is previous column of figure 10.9\n",
    "            # self.Obs(obs[t]) is the t_th column of our p(w_i|t_i) matrix\n",
    "            trellis[:, t] = (trellis[:, t-1, None].dot(self.Obs(obs[t]).T) * self.transProb).max(0)\n",
    "            backpt[:, t] = (np.tile(trellis[:, t-1, None], [1, self.N]) * self.transProb).argmax(0)\n",
    "        # termination\n",
    "        tokens = [trellis[:, -1].argmax()]\n",
    "        for i in xrange(len(obs)-1, 0, -1):\n",
    "            tokens.append(backpt[tokens[-1], i])\n",
    "        return tokens[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = decoder.Decode(np.arange(len(sentence)))\n",
    "# result = np.array(pos_tags)[states].tolist()\n",
    "# resultTagged = zip(sentence,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resultTagged)\n",
    "# print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '.'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do/questions\n",
    "- fix p_wi_ti and p_ti_tim1 to include punctuation\n",
    "- create obs matrix and run decoder for each sentence,\n",
    "    - should obs matrix only contain unique words, ie 'janet loves janet' would only have 2 columns?\n",
    "    - for each sentence, get error??\n",
    "- testing? p_wi_ti, p_ti_tim1, obs matrix and transition matrix should all be fixed from using the training data? This means I have to create obs matrix for the whole text in the training data?? So basically, in the training file for sentence one, I map each word to a state and in the text file, I find the state of a given word and use that? if a word is new??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as train:\n",
    "    lines = train.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from each word to a state\n",
    "# with open(\"POS.train\", 'r') as train:\n",
    "#     for line in train:\n",
    "words = []\n",
    "for line in lines:\n",
    "    for pair in line.split():\n",
    "        words.append(pair.split('/')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ordered word mapping to observation matrix (mainly for testin)\n",
    "from collections import OrderedDict\n",
    "\n",
    "word_map = OrderedDict()\n",
    "\n",
    "ind = 0\n",
    "for word in words:\n",
    "    if word not in word_map:\n",
    "        word_map[word] = ind\n",
    "        ind += 1\n",
    "\n",
    "word_map['unknown/new word'] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_map['unknown/new word'], o_mat.shape)\n",
    "# pos_word_prob.most_common()[-5:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mat = np.zeros(shape=(len(tag_list), len(word_map.keys())))\n",
    "num_to_share = 0\n",
    "for word in word_map.keys():\n",
    "    for pos in tag_list:\n",
    "        wi_ti = word + '/' + pos\n",
    "        if pos_word_prob[wi_ti] == 0:\n",
    "            num_to_share += 1\n",
    "\n",
    "for col_ind, word in enumerate(word_map.keys()):\n",
    "    for row_ind, pos in enumerate(tag_list):\n",
    "        wi_ti = word + '/' + pos\n",
    "        try:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_map,\n",
    "#                                                             wi_ti, pos_word_prob[wi_ti]))\n",
    "            o_mat[row_ind, col_ind] = pos_word_prob[wi_ti]\n",
    "        except:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             wi_ti, 0))       \n",
    "            pass\n",
    "#             o_mat[row_ind, col_ind] = turing / num_to_share\n",
    "\n",
    "o_mat[:, -1] = turing / num_to_share\n",
    "decoder = Decoder(initialProb=init_state, transProb=trans_p, obsProb=o_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoder(decoder, filename='POS.test'):\n",
    "    result_list = []\n",
    "    truth_counter = 0\n",
    "    num_chances = 0\n",
    "    num_correct = 0\n",
    "    result_list = []\n",
    "    for line in test_lines:\n",
    "    #     print(line)\n",
    "        sentence = []\n",
    "        state = []\n",
    "        # need to do this \n",
    "        for col_ind, txt in enumerate(line.split()):\n",
    "            word = txt.split('/')[0]\n",
    "    #         print('word:state is {}:{}'.format(word, word_map[word]))\n",
    "            try:\n",
    "                state.append(word_map[word])\n",
    "            except: # if we have a new word\n",
    "    #             print('new', word)\n",
    "                state.append(word_map['unknown/new word'])\n",
    "            sentence.append(word)\n",
    "        states = decoder.Decode(np.array(state))\n",
    "        result = np.array(tag_list)[states].tolist()\n",
    "        resultTagged = zip(sentence,result)\n",
    "        result_list.append(resultTagged)\n",
    "    #     print(resultTagged)\n",
    "    #     print(line)\n",
    "        for val in result:\n",
    "            num_chances += 1\n",
    "            if test_unigram[truth_counter] == 'start':\n",
    "                truth_counter += 1\n",
    "            if val == test_unigram[truth_counter]:\n",
    "                num_correct += 1\n",
    "\n",
    "\n",
    "\n",
    "            truth_counter += 1\n",
    "    accuracy = num_correct / num_chances\n",
    "    return accuracy, result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, rslt_list = run_decoder(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205298013245033"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('POS.test.out', 'w') as outfile:\n",
    "    for sent in rslt_list:\n",
    "        for pair in sent:\n",
    "            outfile.write('%s/%s ' %pair)\n",
    "        outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
