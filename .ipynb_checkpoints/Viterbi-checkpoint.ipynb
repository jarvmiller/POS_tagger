{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"POS.train.large\"\n",
    "\n",
    "# unigram = []\n",
    "\n",
    "def create_unigram(filename):\n",
    "    bigram = []\n",
    "    bigram_count= {}\n",
    "    i = 0\n",
    "    unigram = []\n",
    "    with open (filename,'r') as training:\n",
    "        lines = training.readlines()\n",
    "        start = len(lines)\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            line.append('start')\n",
    "            for item in line:\n",
    "                tag = item.split('/')[-1]\n",
    "                # if it's an ambiguous tag we get the second one\n",
    "                if \"|\" in tag:\n",
    "                    tag = tag.split(\"|\")[-1]\n",
    "\n",
    "                unigram.append(tag)\n",
    "#         return unigram\n",
    "\n",
    "    for word in unigram:\n",
    "\n",
    "        if i < len(unigram)-1:\n",
    "            bigram.append(unigram[i] + ' '+unigram[i+1])\n",
    "            i+=1\n",
    "\n",
    "    new_bigram = [x for x in bigram if not x.endswith(\"start\")]\n",
    "\n",
    "    for item in new_bigram:       \n",
    "        if item not in bigram_count:\n",
    "            bigram_count[item]=1\n",
    "        else:\n",
    "            bigram_count[item]+=1\n",
    "\n",
    "    # tag_frequency, pos_word_prob \n",
    "    pos = []\n",
    "    with open (filename,'r') as training:\n",
    "        string_r = training.read()\n",
    "\n",
    "        splited_string = string_r.split()\n",
    "        for item in splited_string:\n",
    "            pos.append(item.split('/')[-1]) #Choose the last one when there are multiple slashes.\n",
    "\n",
    "        tag_frequency = Counter(pos)\n",
    "        tag_frequency.update({'start':start})\n",
    "\n",
    "        pos_word_count = Counter(splited_string)\n",
    "        pos_word_prob = pos_word_count.copy()\n",
    "\n",
    "        for item in pos_word_count:\n",
    "            pos_word_prob[item] = pos_word_count[item]/float(tag_frequency[item.split('/')[-1]])\n",
    "\n",
    "        # bigram_prob\n",
    "    bigram_prob = {}\n",
    "    for key,value in bigram_count.items():\n",
    "        bigram_prob[key]=value/float(tag_frequency[key.split()[0]])\n",
    "\n",
    "    return unigram, bigram_prob, bigram_count, pos_word_count, pos_word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram, bigram_prob, bigram_count, pos_word_count, pos_word_prob = create_unigram(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 0\n",
    "for key, val in pos_word_count.items():\n",
    "    if val == 1:\n",
    "        N1 += 1\n",
    "turing = N1 / sum(pos_word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram, bigram_prob, lines = parse_train()\n",
    "\n",
    "all_tags = []\n",
    "for key in bigram_prob.keys(): # in order i-1 i\n",
    "    all_tags.extend(key.split())\n",
    "\n",
    "# store unique tags\n",
    "tag_list = list(set(all_tags))\n",
    "# bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the testing file\n",
    "test_unigram = []\n",
    "test_path = 'POS.test'\n",
    "with open (test_path,'r') as testing:\n",
    "    test_lines = testing.readlines()\n",
    "    start = len(test_lines)\n",
    "\n",
    "    for line in test_lines:\n",
    "        line = line.split()\n",
    "        line.append('start')\n",
    "        for item in line:\n",
    "            tag = item.split('/')[-1]\n",
    "            # if it's an ambiguous tag we get the second one\n",
    "            if \"|\" in tag:\n",
    "                tag = tag.split(\"|\")[-1]\n",
    "\n",
    "            test_unigram.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49, 49), 49)\n"
     ]
    }
   ],
   "source": [
    "trans_p = np.zeros(shape=(len(tag_list), len(tag_list)))\n",
    "print(trans_p.shape, len(trans_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for col_ind, tag in enumerate(tag_list):\n",
    "    for row_ind, cond_tag in enumerate(tag_list):\n",
    "\n",
    "        ti_tim1 = cond_tag + ' ' + tag\n",
    "        try:\n",
    "#             print('trans_p at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             ti_tim1, bigram_prob[ti_tim1]))\n",
    "            trans_p[row_ind, col_ind] = bigram_prob[ti_tim1]\n",
    "        except:\n",
    "#             num_to_share += 1\n",
    "                pass\n",
    "#             print('trans_p at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             ti_tim1, 0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = np.zeros(shape=(len(tag_list),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, tag in enumerate(tag_list):\n",
    "    try:\n",
    "#         print('init_state at {}, is {} w/ val {}'.format(ind, 'start' + \" \"+ tag,\n",
    "#                                                          bigram_prob['start' + ' ' + tag]))       \n",
    "        init_state[ind] = bigram_prob['start' + ' ' + tag]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, initialProb, transProb, obsProb):\n",
    "        self.N = initialProb.shape[0]\n",
    "        self.initialProb = initialProb\n",
    "        self.transProb = transProb\n",
    "        self.obsProb = obsProb\n",
    "        assert self.initialProb.shape == (self.N, 1)\n",
    "        assert self.transProb.shape == (self.N, self.N)\n",
    "        assert self.obsProb.shape[0] == self.N\n",
    "\n",
    "    def Obs(self, obs):\n",
    "        return self.obsProb[:, obs, None]\n",
    "\n",
    "    def Decode(self, obs):\n",
    "        trellis = np.zeros((self.N, len(obs)))\n",
    "        backpt = np.ones((self.N, len(obs)), 'int32') * -1\n",
    "\n",
    "        # initialization\n",
    "        # first row of trans matrix time first column of \n",
    "        # emission matrix\n",
    "        trellis[:, 0] = np.squeeze(self.initialProb * self.Obs(obs[0]))\n",
    "\n",
    "        for t in xrange(1, len(obs)):\n",
    "            # trellis[:, t-1, None] is previous column of figure 10.9\n",
    "            # self.Obs(obs[t]) is the t_th column of our p(w_i|t_i) matrix\n",
    "            trellis[:, t] = (trellis[:, t-1, None].dot(self.Obs(obs[t]).T) * self.transProb).max(0)\n",
    "            backpt[:, t] = (np.tile(trellis[:, t-1, None], [1, self.N]) * self.transProb).argmax(0)\n",
    "        # termination\n",
    "        tokens = [trellis[:, -1].argmax()]\n",
    "        for i in xrange(len(obs)-1, 0, -1):\n",
    "            tokens.append(backpt[tokens[-1], i])\n",
    "        return tokens[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do/questions\n",
    "- fix p_wi_ti and p_ti_tim1 to include punctuation\n",
    "- create obs matrix and run decoder for each sentence,\n",
    "    - should obs matrix only contain unique words, ie 'janet loves janet' would only have 2 columns?\n",
    "    - for each sentence, get error??\n",
    "- testing? p_wi_ti, p_ti_tim1, obs matrix and transition matrix should all be fixed from using the training data? This means I have to create obs matrix for the whole text in the training data?? So basically, in the training file for sentence one, I map each word to a state and in the text file, I find the state of a given word and use that? if a word is new??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as train:\n",
    "    lines = train.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = []\n",
    "for line in lines:\n",
    "    for pair in line.split():\n",
    "        words.append(pair.split('/')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ordered word mapping to observation matrix (mainly for testin)\n",
    "from collections import OrderedDict\n",
    "\n",
    "word_map = OrderedDict()\n",
    "\n",
    "ind = 0\n",
    "for word in words:\n",
    "    if word not in word_map:\n",
    "        word_map[word] = ind\n",
    "        ind += 1\n",
    "\n",
    "word_map['unknown/new word'] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mat = np.zeros(shape=(len(tag_list), len(word_map.keys())))\n",
    "num_to_share = 0\n",
    "for word in word_map.keys():\n",
    "    for pos in tag_list:\n",
    "        wi_ti = word + '/' + pos\n",
    "        if pos_word_prob[wi_ti] == 0:\n",
    "            num_to_share += 1\n",
    "\n",
    "for col_ind, word in enumerate(word_map.keys()):\n",
    "    for row_ind, pos in enumerate(tag_list):\n",
    "        wi_ti = word + '/' + pos\n",
    "        try:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_map,\n",
    "#                                                             wi_ti, pos_word_prob[wi_ti]))\n",
    "            o_mat[row_ind, col_ind] = pos_word_prob[wi_ti]\n",
    "        except:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             wi_ti, 0))       \n",
    "            pass\n",
    "#             o_mat[row_ind, col_ind] = turing / num_to_share\n",
    "\n",
    "o_mat[:, -1] = turing / num_to_share\n",
    "decoder = Decoder(initialProb=init_state, transProb=trans_p, obsProb=o_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoder(decoder, filename='POS.test'):\n",
    "    result_list = []\n",
    "    truth_counter = 0\n",
    "    num_chances = 0\n",
    "    num_correct = 0\n",
    "    result_list = []\n",
    "    for line in test_lines:\n",
    "    #     print(line)\n",
    "        sentence = []\n",
    "        state = []\n",
    "        # need to do this \n",
    "        for col_ind, txt in enumerate(line.split()):\n",
    "            word = txt.split('/')[0]\n",
    "    #         print('word:state is {}:{}'.format(word, word_map[word]))\n",
    "            try:\n",
    "                state.append(word_map[word])\n",
    "            except: # if we have a new word\n",
    "    #             print('new', word)\n",
    "                state.append(word_map['unknown/new word'])\n",
    "            sentence.append(word)\n",
    "        states = decoder.Decode(np.array(state))\n",
    "        result = np.array(tag_list)[states].tolist()\n",
    "        resultTagged = zip(sentence,result)\n",
    "        result_list.append(resultTagged)\n",
    "    #     print(resultTagged)\n",
    "    #     print(line)\n",
    "        for val in result:\n",
    "            num_chances += 1\n",
    "            if test_unigram[truth_counter] == 'start':\n",
    "                truth_counter += 1\n",
    "            if val == test_unigram[truth_counter]:\n",
    "                num_correct += 1\n",
    "\n",
    "\n",
    "\n",
    "            truth_counter += 1\n",
    "    accuracy = num_correct / num_chances\n",
    "    return accuracy, result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, rslt_list = run_decoder(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554027998994048"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('POS.test.out', 'w') as outfile:\n",
    "#     for sent in rslt_list:\n",
    "#         for pair in sent:\n",
    "#             outfile.write('%s/%s ' %pair)\n",
    "#         outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mat = np.zeros(shape=(len(tag_list), len(word_map.keys())))\n",
    "for col_ind, word in enumerate(word_map.keys()):\n",
    "    for row_ind, pos in enumerate(tag_list):\n",
    "        wi_ti = word + '/' + pos\n",
    "        try:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_map,\n",
    "#                                                             wi_ti, pos_word_prob[wi_ti]))\n",
    "            obs_mat[row_ind, col_ind] = pos_word_count[wi_ti]\n",
    "        except:\n",
    "#             print('obs_mat at {},{} is {} w/ val {}'.format(row_ind, col_ind,\n",
    "#                                                             wi_ti, 0))       \n",
    "            pass\n",
    "#             o_mat[row_ind, col_ind] = turing / num_to_share\n",
    "\n",
    "# for unknown words, choose from top 5 tags randomly\n",
    "pos_mean = obs_mat.sum(axis=1) # should i use the sum or the mean?\n",
    "best_five = np.argpartition(pos_mean, -4)[-4:]\n",
    "pos_choices = [tag_list[x] for x in best_five]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "num_chances = 0\n",
    "num_correct = 0\n",
    "\n",
    "for line in test_lines:\n",
    "#     print(line)\n",
    "    sentence = []\n",
    "    state = []\n",
    "    rslt = []\n",
    "    # need to do this \n",
    "    for col_ind, txt in enumerate(line.split()):\n",
    "        word = txt.split('/')[0]\n",
    "        try:\n",
    "            state.append(word_map[word])\n",
    "#             print('word:state is {}:{}'.format(word, word_map[word]))\n",
    "        except: # if we have a new word\n",
    "#             print('word:state is {}:{}'.format(word, word_map['unknown/new word']))\n",
    "            state.append(word_map['unknown/new word'])\n",
    "        sentence.append(word)\n",
    "    for val in state:\n",
    "        if val == 21162:\n",
    "            rslt.append(random.choice(pos_choices))\n",
    "        else:\n",
    "            rslt.append(tag_list[obs_mat[:,val].argmax()])\n",
    "    rsltTagged = zip(sentence, rslt)\n",
    "    for val in rslt:\n",
    "        num_chances += 1\n",
    "        if test_unigram[counter] == 'start':\n",
    "            counter += 1\n",
    "        if val == test_unigram[counter]:\n",
    "            num_correct += 1\n",
    "\n",
    "\n",
    "\n",
    "        counter += 1\n",
    "    accuracy = num_correct / num_chances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9190208735015508"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
